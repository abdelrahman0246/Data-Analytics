{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "d1 = open('d1.txt','r')\n",
    "d2 = open('d2.txt','r')\n",
    "d3 = open('d3.txt','r')\n",
    "d4 = open('d4.txt','r')\n",
    "d5 = open('d5.txt','r')\n",
    "dq = open('d_query.txt','r')\n",
    "def get_words(fileid):\n",
    "    words = []\n",
    "    temp = []\n",
    "    while(True):\n",
    "        r = fileid.read(1)\n",
    "        if(r.isalpha()):\n",
    "            temp.append(r)\n",
    "        else:\n",
    "            word = ''.join(temp)\n",
    "            if(word!=\"\"):\n",
    "                in_lowercase = word.casefold()\n",
    "                words.append(in_lowercase)\n",
    "                temp = []\n",
    "        if(r==''):\n",
    "            break\n",
    "    return words\n",
    "D1 = get_words(d1)\n",
    "D2 = get_words(d2)\n",
    "D3 = get_words(d3)\n",
    "D4 = get_words(d4)\n",
    "D5 = get_words(d5)\n",
    "DQ = get_words(dq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [DQ,D1,D2,D3,D4,D5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abood\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import math\n",
    "from nltk import FreqDist;\n",
    "import numpy as np\n",
    "\n",
    "input_files = ['d1.txt', 'd2.txt', 'd3.txt', 'd4.txt', 'd5.txt'];\n",
    "all_words = [];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract words from documents\n",
    "for files in input_files:\n",
    "  f = open(files, 'r');\n",
    "  temp = f.read();\n",
    "  f.close();\n",
    "  temp1 = temp.translate(str.maketrans(\"\",\"\", string.punctuation));\n",
    "  words = nltk.word_tokenize(temp1);\n",
    "  all_words = all_words + words;\n",
    "  words = [];\n",
    "\n",
    "all_words = [i.lower() for i in all_words];\n",
    "all_words.sort();\n",
    "\n",
    "#print len(all_words)\n",
    "dictofwords = list(set(all_words));\n",
    "dictofwords.sort();\n",
    "#print (dictofwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert documents to vectors\n",
    "query = ['d_query.txt'];\n",
    "all_files = input_files + query;\n",
    "matrixofdocs = np.zeros((len(dictofwords), len(all_files)));\n",
    "\n",
    "row = 0;\n",
    "col = 0;\n",
    "\n",
    "for i in all_files:\n",
    "   f = open(i, 'r');\n",
    "   temp = f.read();\n",
    "   f.close();\n",
    "\n",
    "   temp = temp.translate(str.maketrans(\"\",\"\",string.punctuation));\n",
    "\n",
    "   words = nltk.word_tokenize(temp);\n",
    "   words.sort();\n",
    "   words = [w.lower() for w in words];\n",
    "\n",
    "   dist = FreqDist(words)\n",
    "   row = 0;\n",
    "\n",
    "   for j in dictofwords:\n",
    "    if j not in words:\n",
    "      matrixofdocs[row][col] = 0;\n",
    "    else:\n",
    "      matrixofdocs[row][col] += dist[j];\n",
    "    row += 1;\n",
    "\n",
    "   col += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import word2vec\n",
    "\n",
    "\n",
    "def wordCount(document):\n",
    "    \n",
    "    wordvec = {}\n",
    "    for i in document:\n",
    "        if i in wordvec:\n",
    "            wordvec[i] += 1\n",
    "        else:\n",
    "            wordvec[i] = 1\n",
    "    \n",
    "    # Computes tf for each word\n",
    "   # for word in reviewTFDict:\n",
    "      #  reviewTFDict[word] = reviewTFDict[word] / len(review)\n",
    "  #  return reviewTFDict\n",
    "    #for i in wordvec:\n",
    "        #wordvec[i] = wordvec[i] / len(document)\n",
    "    \n",
    "    return wordvec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computetf (document):\n",
    "    \n",
    "    wordvec = {}\n",
    "    for i in document:\n",
    "        if i in wordvec:\n",
    "            wordvec[i] += 1\n",
    "        else:\n",
    "            wordvec[i] = 1\n",
    "    \n",
    "    \n",
    "    for i in wordvec:\n",
    "        wordvec[i] = wordvec[i] / len(document)\n",
    "    \n",
    "    \n",
    "    return wordvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1tf= computetf(D1)\n",
    "D2tf= computetf(D2)\n",
    "D3tf= computetf(D3)\n",
    "D4tf= computetf(D4)\n",
    "D5tf= computetf(D5)\n",
    "DQtf= computetf(DQ)\n",
    "tfdocs = np.array([DQtf,D1tf,D2tf,D3tf,D4tf,D5tf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountDict():\n",
    "    countDict = {}\n",
    "    #for i in dictofwords:\n",
    "    for document in tfdocs:\n",
    "            \n",
    "        for word in document:\n",
    "            if word in countDict:\n",
    "                countDict[word] += 1\n",
    "            else:\n",
    "                countDict[word] = 1\n",
    "    return countDict\n",
    "        \n",
    "countDict = CountDict()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDFDict():\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in the\n",
    "    dataset and whose values are their corresponding idf.\n",
    "    \"\"\"\n",
    "    idfDict = {}\n",
    "    for word in countDict:\n",
    "       # print(countDict[word])\n",
    "        if countDict[word] > 0:\n",
    "            idfDict[word] = 1.0 + math.log(float(len(documents)) / countDict[word])\n",
    "        else:\n",
    "             idfDict[word] = 1.0\n",
    "           \n",
    "        \n",
    "    return idfDict\n",
    "  \n",
    "  #Stores the idf dictionary\n",
    "idfDict = computeIDFDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeReviewTFIDFDict(document):\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in the\n",
    "    review and whose values are their corresponding tfidf.\n",
    "    \"\"\"\n",
    "    reviewTFIDFDict = {}\n",
    "    wordvec = computetf(document)\n",
    "    #For each word in the review, we multiply its tf and its idf.\n",
    "    for word in wordvec:\n",
    "        reviewTFIDFDict[word] = wordvec[word] * idfDict[word]\n",
    "    return reviewTFIDFDict\n",
    "\n",
    "  #Stores the TF-IDF dictionaries\n",
    "tfidfDict = [computeReviewTFIDFDict(wordvec) for wordvec in tfdocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDict = sorted(countDict.keys())\n",
    "#print(wordDict)\n",
    "def computeTFIDFVector(document):\n",
    "    tfidfVector = [0.0] * len(wordDict)\n",
    "\n",
    "      # For each unique word, if it is in the review, store its TF-IDF value.\n",
    "    for i, word in enumerate(wordDict):\n",
    "        if word in document:\n",
    "            tfidfVector[i] = document[word]\n",
    "    return tfidfVector\n",
    "\n",
    "tfidfVector = [computeTFIDFVector(document) for document in tfidfDict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = np.zeros((6,6))\n",
    "#print(cos)\n",
    "#benchot = [][]\n",
    "row = 0;\n",
    "col = 0;\n",
    "for i in range (len(tfdocs)):\n",
    "    \n",
    "    doc = tfdocs[i]\n",
    "    TFIDF1 = computeTFIDFVector(doc)\n",
    "    \n",
    "    for j in range (len(tfdocs)):\n",
    "        doc2 = tfdocs[j]\n",
    "        TFIDF2 = computeTFIDFVector(doc2)\n",
    "        cos[i][j] += round(cosine_similarity(TFIDF1, TFIDF2),3)\n",
    "        j+=1\n",
    "i+=1        \n",
    "       \n",
    "    #arr_flat = np.append(cos,sim)\n",
    "#print(benchot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.152, 0.119, 0.116, 0.312, 0.259],\n",
       "       [0.152, 1.   , 0.767, 0.667, 0.568, 0.641],\n",
       "       [0.119, 0.767, 1.   , 0.72 , 0.561, 0.567],\n",
       "       [0.116, 0.667, 0.72 , 1.   , 0.643, 0.568],\n",
       "       [0.312, 0.568, 0.561, 0.643, 1.   , 0.527],\n",
       "       [0.259, 0.641, 0.567, 0.568, 0.527, 1.   ]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
